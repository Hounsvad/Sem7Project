version: "3.5"

services:
  dataaccessor:
    image: ghcr.io/hounsvad/sem7dataaccessor:latest
    restart: always
    ports:
      - 4080:80
    environment:
      HIVE_HOSTNAME: "hive"
      
  namenode: #Master (metadata/chunk)
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    volumes:
      - ./volumes/hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 50070:50070

  datanode1: #Slave1
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    restart: always
    volumes:
      - ./volumes/hadoop_datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

  datanode2: #Slave2
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: always
    volumes:
      - ./volumes/hadoop_datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

  datanode3: #Slave3
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    restart: always
    volumes:
      - ./volumes/hadoop_datanode3:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
      
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
      
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 hive-metastore-postgresql:5432"
      
  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0

  kafka:
    image: confluentinc/cp-kafka:6.2.0
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_PARTITION_ASSIGNMENT_STRATEGY: "org.apache.kafka.clients.consumer.RoundRobinAssignor"

  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"

  kowl:
    image: quay.io/cloudhut/kowl:master
    restart: always
    environment:
      KAFKA_BROKERS: "kafka:9092"
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
    ports:
      - "4082:8080"

  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    environment:
      INIT_DAEMON_STEP: "setup_spark"
      SPARK_MASTER_PORT: "7077"
    ports:
      - "8080:8080"

  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
    ports:
      - "8081:8081"

  spark-worker2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
    ports:
      - "8082:8081"

  spark-worker3:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
    ports:
      - "8083:8081"

  
  ingest:
    image: ghcr.io/hounsvad/sem7ingest:latest
    ports:
      - "4081:81"
    env_file:
      - ./ingest.env
    volumes:
      - ./main.py:/app/PythonProcessor.py
      - ./ingest:/tmp
    

volumes:
  hbase_data:
  hbase_zookeeper_data: